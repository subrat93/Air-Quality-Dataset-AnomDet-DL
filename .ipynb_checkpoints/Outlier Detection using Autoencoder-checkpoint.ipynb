{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('air_quality_precprocessed_with_outlier_status.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6934, 13)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "The Data is already preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.047480</td>\n",
       "      <td>0.996190</td>\n",
       "      <td>0.221753</td>\n",
       "      <td>0.652196</td>\n",
       "      <td>0.353498</td>\n",
       "      <td>1.022904</td>\n",
       "      <td>0.418512</td>\n",
       "      <td>0.869666</td>\n",
       "      <td>0.802365</td>\n",
       "      <td>-0.005854</td>\n",
       "      <td>0.318106</td>\n",
       "      <td>-0.479861</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.208560</td>\n",
       "      <td>0.858273</td>\n",
       "      <td>0.060275</td>\n",
       "      <td>0.451107</td>\n",
       "      <td>0.108475</td>\n",
       "      <td>1.317484</td>\n",
       "      <td>0.296794</td>\n",
       "      <td>0.669960</td>\n",
       "      <td>0.267960</td>\n",
       "      <td>-0.021763</td>\n",
       "      <td>0.301371</td>\n",
       "      <td>-0.505131</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.150044</td>\n",
       "      <td>1.081374</td>\n",
       "      <td>0.030499</td>\n",
       "      <td>0.415751</td>\n",
       "      <td>0.231931</td>\n",
       "      <td>1.232605</td>\n",
       "      <td>0.423728</td>\n",
       "      <td>0.663953</td>\n",
       "      <td>0.452113</td>\n",
       "      <td>-0.101096</td>\n",
       "      <td>0.384922</td>\n",
       "      <td>-0.485709</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.150044</td>\n",
       "      <td>1.028641</td>\n",
       "      <td>0.045549</td>\n",
       "      <td>0.435639</td>\n",
       "      <td>0.371727</td>\n",
       "      <td>1.112776</td>\n",
       "      <td>0.463880</td>\n",
       "      <td>0.707498</td>\n",
       "      <td>0.685013</td>\n",
       "      <td>-0.157188</td>\n",
       "      <td>0.455884</td>\n",
       "      <td>-0.458148</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.345560</td>\n",
       "      <td>0.817710</td>\n",
       "      <td>-0.192326</td>\n",
       "      <td>0.188144</td>\n",
       "      <td>0.231931</td>\n",
       "      <td>1.394874</td>\n",
       "      <td>0.434024</td>\n",
       "      <td>0.566352</td>\n",
       "      <td>0.517109</td>\n",
       "      <td>-0.144336</td>\n",
       "      <td>0.451379</td>\n",
       "      <td>-0.456602</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0 -0.047480  0.996190  0.221753  0.652196  0.353498  1.022904  0.418512   \n",
       "1 -0.208560  0.858273  0.060275  0.451107  0.108475  1.317484  0.296794   \n",
       "2 -0.150044  1.081374  0.030499  0.415751  0.231931  1.232605  0.423728   \n",
       "3 -0.150044  1.028641  0.045549  0.435639  0.371727  1.112776  0.463880   \n",
       "4 -0.345560  0.817710 -0.192326  0.188144  0.231931  1.394874  0.434024   \n",
       "\n",
       "          7         8         9        10        11  outlier  \n",
       "0  0.869666  0.802365 -0.005854  0.318106 -0.479861      0.0  \n",
       "1  0.669960  0.267960 -0.021763  0.301371 -0.505131      0.0  \n",
       "2  0.663953  0.452113 -0.101096  0.384922 -0.485709      0.0  \n",
       "3  0.707498  0.685013 -0.157188  0.455884 -0.458148      0.0  \n",
       "4  0.566352  0.517109 -0.144336  0.451379 -0.456602      0.0  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6934.000000</td>\n",
       "      <td>6934.000000</td>\n",
       "      <td>6934.000000</td>\n",
       "      <td>6934.000000</td>\n",
       "      <td>6934.000000</td>\n",
       "      <td>6934.000000</td>\n",
       "      <td>6934.000000</td>\n",
       "      <td>6934.000000</td>\n",
       "      <td>6934.000000</td>\n",
       "      <td>6934.000000</td>\n",
       "      <td>6934.000000</td>\n",
       "      <td>6934.000000</td>\n",
       "      <td>4923.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.292025</td>\n",
       "      <td>0.509015</td>\n",
       "      <td>-0.045139</td>\n",
       "      <td>0.459209</td>\n",
       "      <td>0.392697</td>\n",
       "      <td>0.423236</td>\n",
       "      <td>0.365544</td>\n",
       "      <td>0.510895</td>\n",
       "      <td>0.423219</td>\n",
       "      <td>0.074484</td>\n",
       "      <td>0.267273</td>\n",
       "      <td>-0.378464</td>\n",
       "      <td>0.048548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.446254</td>\n",
       "      <td>0.445941</td>\n",
       "      <td>0.550490</td>\n",
       "      <td>0.584409</td>\n",
       "      <td>0.443804</td>\n",
       "      <td>0.628107</td>\n",
       "      <td>0.279644</td>\n",
       "      <td>0.531043</td>\n",
       "      <td>0.734150</td>\n",
       "      <td>0.447387</td>\n",
       "      <td>0.279682</td>\n",
       "      <td>0.271416</td>\n",
       "      <td>0.214942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.047804</td>\n",
       "      <td>-1.757778</td>\n",
       "      <td>-2.576025</td>\n",
       "      <td>-1.654468</td>\n",
       "      <td>-1.915124</td>\n",
       "      <td>-1.607960</td>\n",
       "      <td>-1.969821</td>\n",
       "      <td>-1.667734</td>\n",
       "      <td>-1.483023</td>\n",
       "      <td>-3.015439</td>\n",
       "      <td>-1.785292</td>\n",
       "      <td>-1.294135</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.575605</td>\n",
       "      <td>0.179338</td>\n",
       "      <td>-0.385807</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>0.108475</td>\n",
       "      <td>-0.012494</td>\n",
       "      <td>0.206606</td>\n",
       "      <td>0.142913</td>\n",
       "      <td>-0.114789</td>\n",
       "      <td>-0.142751</td>\n",
       "      <td>0.098612</td>\n",
       "      <td>-0.530005</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.240052</td>\n",
       "      <td>0.438439</td>\n",
       "      <td>0.015112</td>\n",
       "      <td>0.398072</td>\n",
       "      <td>0.411902</td>\n",
       "      <td>0.346369</td>\n",
       "      <td>0.402582</td>\n",
       "      <td>0.518302</td>\n",
       "      <td>0.331150</td>\n",
       "      <td>0.149097</td>\n",
       "      <td>0.322909</td>\n",
       "      <td>-0.345226</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.019563</td>\n",
       "      <td>0.781202</td>\n",
       "      <td>0.361768</td>\n",
       "      <td>0.850523</td>\n",
       "      <td>0.713593</td>\n",
       "      <td>0.745800</td>\n",
       "      <td>0.553751</td>\n",
       "      <td>0.856152</td>\n",
       "      <td>0.901212</td>\n",
       "      <td>0.390292</td>\n",
       "      <td>0.481219</td>\n",
       "      <td>-0.187629</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.664996</td>\n",
       "      <td>2.375357</td>\n",
       "      <td>3.320688</td>\n",
       "      <td>3.233209</td>\n",
       "      <td>1.476368</td>\n",
       "      <td>5.084609</td>\n",
       "      <td>1.765316</td>\n",
       "      <td>2.495852</td>\n",
       "      <td>3.068170</td>\n",
       "      <td>3.126952</td>\n",
       "      <td>2.414449</td>\n",
       "      <td>3.743314</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2            3            4  \\\n",
       "count  6934.000000  6934.000000  6934.000000  6934.000000  6934.000000   \n",
       "mean     -0.292025     0.509015    -0.045139     0.459209     0.392697   \n",
       "std       0.446254     0.445941     0.550490     0.584409     0.443804   \n",
       "min      -2.047804    -1.757778    -2.576025    -1.654468    -1.915124   \n",
       "25%      -0.575605     0.179338    -0.385807     0.020202     0.108475   \n",
       "50%      -0.240052     0.438439     0.015112     0.398072     0.411902   \n",
       "75%       0.019563     0.781202     0.361768     0.850523     0.713593   \n",
       "max       3.664996     2.375357     3.320688     3.233209     1.476368   \n",
       "\n",
       "                 5            6            7            8            9  \\\n",
       "count  6934.000000  6934.000000  6934.000000  6934.000000  6934.000000   \n",
       "mean      0.423236     0.365544     0.510895     0.423219     0.074484   \n",
       "std       0.628107     0.279644     0.531043     0.734150     0.447387   \n",
       "min      -1.607960    -1.969821    -1.667734    -1.483023    -3.015439   \n",
       "25%      -0.012494     0.206606     0.142913    -0.114789    -0.142751   \n",
       "50%       0.346369     0.402582     0.518302     0.331150     0.149097   \n",
       "75%       0.745800     0.553751     0.856152     0.901212     0.390292   \n",
       "max       5.084609     1.765316     2.495852     3.068170     3.126952   \n",
       "\n",
       "                10           11      outlier  \n",
       "count  6934.000000  6934.000000  4923.000000  \n",
       "mean      0.267273    -0.378464     0.048548  \n",
       "std       0.279682     0.271416     0.214942  \n",
       "min      -1.785292    -1.294135     0.000000  \n",
       "25%       0.098612    -0.530005     0.000000  \n",
       "50%       0.322909    -0.345226     0.000000  \n",
       "75%       0.481219    -0.187629     0.000000  \n",
       "max       2.414449     3.743314     1.000000  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6934 entries, 0 to 6933\n",
      "Data columns (total 13 columns):\n",
      "0          6934 non-null float64\n",
      "1          6934 non-null float64\n",
      "2          6934 non-null float64\n",
      "3          6934 non-null float64\n",
      "4          6934 non-null float64\n",
      "5          6934 non-null float64\n",
      "6          6934 non-null float64\n",
      "7          6934 non-null float64\n",
      "8          6934 non-null float64\n",
      "9          6934 non-null float64\n",
      "10         6934 non-null float64\n",
      "11         6934 non-null float64\n",
      "outlier    4923 non-null float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 704.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             0\n",
       "1             0\n",
       "2             0\n",
       "3             0\n",
       "4             0\n",
       "5             0\n",
       "6             0\n",
       "7             0\n",
       "8             0\n",
       "9             0\n",
       "10            0\n",
       "11            0\n",
       "outlier    2011\n",
       "dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Data based on Isolation Forest algo from PYOD--\n",
      "No. of inliers :  4684\n",
      "No. of outliers :  239\n"
     ]
    }
   ],
   "source": [
    "print('--Data based on Isolation Forest algo from PYOD--')\n",
    "print('No. of inliers : ',len(df[df.outlier == 0]))\n",
    "print('No. of outliers : ',len(df[df.outlier == 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into train and test\n",
    "### Split such that train only consists of inliers whereas test contains both inliers and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4684, 13)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inliers = df[df.outlier == 0]\n",
    "df_inliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 13)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df_inliers.iloc[:4000]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(684, 13)\n",
      "(923, 13)\n"
     ]
    }
   ],
   "source": [
    "test = df_inliers.iloc[4000:]\n",
    "print(test.shape)\n",
    "test = test.append(df[df.outlier == 1])\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Input\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_autoencoder_1(X, test, epochs):\n",
    "    autoencoder = Sequential()\n",
    "    autoencoder.add(Dense(512,  activation='relu', input_shape=(12,)))\n",
    "    autoencoder.add(Dense(128,  activation='relu'))\n",
    "    autoencoder.add(Dense(2,    activation='linear', name=\"bottleneck\"))\n",
    "    autoencoder.add(Dense(128,  activation='relu'))\n",
    "    autoencoder.add(Dense(512,  activation='relu'))\n",
    "    autoencoder.add(Dense(12,  activation='sigmoid'))\n",
    "    autoencoder.compile(loss='mean_squared_error', optimizer = Adam())\n",
    "    trained_model = autoencoder.fit(X, X, epochs=epochs, verbose=1, validation_data=(test, test))\n",
    "    encoder = Model(autoencoder.input, autoencoder.get_layer('bottleneck').output)\n",
    "    encoded_data = encoder.predict(X)  # bottleneck representation\n",
    "    decoded_output = autoencoder.predict(X)        # reconstruction\n",
    "    encoding_dim = 2\n",
    "\n",
    "    # return the decoder\n",
    "    encoded_input = Input(shape=(encoding_dim,))\n",
    "    decoder = autoencoder.layers[-3](encoded_input)\n",
    "    decoder = autoencoder.layers[-2](decoder)\n",
    "    decoder = autoencoder.layers[-1](decoder)\n",
    "    decoder = Model(encoded_input, decoder)\n",
    "\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_autoencoder_2(X, test, epochs):\n",
    "\n",
    "    input_size = 12\n",
    "    hidden_size = 128\n",
    "    code_size = 2\n",
    "\n",
    "    input_layer = Input(shape=(input_size,))\n",
    "    hidden_1 = Dense(hidden_size, activation='relu')(input_layer)\n",
    "    hidden_3 = Dense(64, activation='relu')(hidden_1)\n",
    "    code = Dense(code_size, activation='linear', name=\"bottleneck\")(hidden_3)\n",
    "    hidden_4 = Dense(64, activation='relu')(code)\n",
    "    hidden_2 = Dense(hidden_size, activation='relu')(hidden_4)\n",
    "    output_layer = Dense(input_size, activation='sigmoid')(hidden_2)\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath=\"model_autoencoder.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "    # tensorboard = TensorBoard(log_dir='./logs',\n",
    "    #                           histogram_freq=0,\n",
    "    #                           write_graph=True,\n",
    "    #                           write_images=True)\n",
    "\n",
    "    autoencoder = Model(input_layer, output_layer)\n",
    "    autoencoder.compile(loss='mean_squared_error', optimizer = Adam(), metrics=['mae'])\n",
    "    \n",
    "    history = autoencoder.fit(X, X, epochs=epochs, verbose=1, callbacks=[checkpointer], validation_data=(test, test)).history\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right');\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 923 samples\n",
      "Epoch 1/200\n",
      "4000/4000 [==============================] - 0s 95us/step - loss: 0.1800 - mae: 0.3108 - val_loss: 0.1851 - val_mae: 0.3115\n",
      "Epoch 2/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.1031 - mae: 0.2222 - val_loss: 0.1744 - val_mae: 0.2987\n",
      "Epoch 3/200\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.0994 - mae: 0.2152 - val_loss: 0.1608 - val_mae: 0.2794\n",
      "Epoch 4/200\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 0.0959 - mae: 0.2067 - val_loss: 0.1510 - val_mae: 0.2610\n",
      "Epoch 5/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0943 - mae: 0.2026 - val_loss: 0.1470 - val_mae: 0.2546\n",
      "Epoch 6/200\n",
      "4000/4000 [==============================] - 0s 52us/step - loss: 0.0934 - mae: 0.2007 - val_loss: 0.1476 - val_mae: 0.2538\n",
      "Epoch 7/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0926 - mae: 0.1995 - val_loss: 0.1452 - val_mae: 0.2491\n",
      "Epoch 8/200\n",
      "4000/4000 [==============================] - 0s 53us/step - loss: 0.0919 - mae: 0.1985 - val_loss: 0.1421 - val_mae: 0.2437\n",
      "Epoch 9/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0915 - mae: 0.1974 - val_loss: 0.1399 - val_mae: 0.2408\n",
      "Epoch 10/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0911 - mae: 0.1964 - val_loss: 0.1399 - val_mae: 0.2406\n",
      "Epoch 11/200\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.0909 - mae: 0.1954 - val_loss: 0.1382 - val_mae: 0.2383\n",
      "Epoch 12/200\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.0907 - mae: 0.1951 - val_loss: 0.1385 - val_mae: 0.2377\n",
      "Epoch 13/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0905 - mae: 0.1943 - val_loss: 0.1375 - val_mae: 0.2360\n",
      "Epoch 14/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0903 - mae: 0.1938 - val_loss: 0.1364 - val_mae: 0.2340\n",
      "Epoch 15/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0901 - mae: 0.1930 - val_loss: 0.1371 - val_mae: 0.2344\n",
      "Epoch 16/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0900 - mae: 0.1929 - val_loss: 0.1365 - val_mae: 0.2338\n",
      "Epoch 17/200\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.0898 - mae: 0.1923 - val_loss: 0.1357 - val_mae: 0.2330\n",
      "Epoch 18/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0898 - mae: 0.1919 - val_loss: 0.1375 - val_mae: 0.2347\n",
      "Epoch 19/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0897 - mae: 0.1917 - val_loss: 0.1364 - val_mae: 0.2332\n",
      "Epoch 20/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0896 - mae: 0.1915 - val_loss: 0.1369 - val_mae: 0.2343\n",
      "Epoch 21/200\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.0893 - mae: 0.1907 - val_loss: 0.1360 - val_mae: 0.2325\n",
      "Epoch 22/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0893 - mae: 0.1906 - val_loss: 0.1364 - val_mae: 0.2348\n",
      "Epoch 23/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0892 - mae: 0.1902 - val_loss: 0.1379 - val_mae: 0.2361\n",
      "Epoch 24/200\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 0.0892 - mae: 0.1901 - val_loss: 0.1363 - val_mae: 0.2340\n",
      "Epoch 25/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0890 - mae: 0.1894 - val_loss: 0.1359 - val_mae: 0.2322\n",
      "Epoch 26/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0888 - mae: 0.1889 - val_loss: 0.1360 - val_mae: 0.2320\n",
      "Epoch 27/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0888 - mae: 0.1889 - val_loss: 0.1361 - val_mae: 0.2330\n",
      "Epoch 28/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0887 - mae: 0.1887 - val_loss: 0.1364 - val_mae: 0.2339\n",
      "Epoch 29/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0887 - mae: 0.1886 - val_loss: 0.1380 - val_mae: 0.2360\n",
      "Epoch 30/200\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 0.0887 - mae: 0.1884 - val_loss: 0.1360 - val_mae: 0.2338\n",
      "Epoch 31/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0885 - mae: 0.1881 - val_loss: 0.1348 - val_mae: 0.2309\n",
      "Epoch 32/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0884 - mae: 0.1876 - val_loss: 0.1352 - val_mae: 0.2305\n",
      "Epoch 33/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0884 - mae: 0.1875 - val_loss: 0.1352 - val_mae: 0.2311\n",
      "Epoch 34/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0883 - mae: 0.1871 - val_loss: 0.1340 - val_mae: 0.2289\n",
      "Epoch 35/200\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 0.0883 - mae: 0.1871 - val_loss: 0.1350 - val_mae: 0.2309\n",
      "Epoch 36/200\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 0.0882 - mae: 0.1869 - val_loss: 0.1358 - val_mae: 0.2318\n",
      "Epoch 37/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0881 - mae: 0.1865 - val_loss: 0.1345 - val_mae: 0.2297\n",
      "Epoch 38/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0880 - mae: 0.1865 - val_loss: 0.1353 - val_mae: 0.2304\n",
      "Epoch 39/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0880 - mae: 0.1862 - val_loss: 0.1370 - val_mae: 0.2339\n",
      "Epoch 40/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0879 - mae: 0.1861 - val_loss: 0.1356 - val_mae: 0.2322\n",
      "Epoch 41/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0878 - mae: 0.1858 - val_loss: 0.1352 - val_mae: 0.2292\n",
      "Epoch 42/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0879 - mae: 0.1857 - val_loss: 0.1346 - val_mae: 0.2293\n",
      "Epoch 43/200\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.0879 - mae: 0.1858 - val_loss: 0.1358 - val_mae: 0.2330\n",
      "Epoch 44/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0876 - mae: 0.1851 - val_loss: 0.1344 - val_mae: 0.2296\n",
      "Epoch 45/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0876 - mae: 0.1850 - val_loss: 0.1346 - val_mae: 0.2284\n",
      "Epoch 46/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0876 - mae: 0.1850 - val_loss: 0.1337 - val_mae: 0.2278\n",
      "Epoch 47/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0875 - mae: 0.1849 - val_loss: 0.1348 - val_mae: 0.2317\n",
      "Epoch 48/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0874 - mae: 0.1845 - val_loss: 0.1340 - val_mae: 0.2295\n",
      "Epoch 49/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0874 - mae: 0.1844 - val_loss: 0.1340 - val_mae: 0.2288\n",
      "Epoch 50/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0873 - mae: 0.1842 - val_loss: 0.1341 - val_mae: 0.2290\n",
      "Epoch 51/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0872 - mae: 0.1840 - val_loss: 0.1347 - val_mae: 0.2309\n",
      "Epoch 52/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0873 - mae: 0.1843 - val_loss: 0.1339 - val_mae: 0.2297\n",
      "Epoch 53/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0873 - mae: 0.1843 - val_loss: 0.1342 - val_mae: 0.2307\n",
      "Epoch 54/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0872 - mae: 0.1838 - val_loss: 0.1342 - val_mae: 0.2298\n",
      "Epoch 55/200\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.0873 - mae: 0.1841 - val_loss: 0.1357 - val_mae: 0.2326\n",
      "Epoch 56/200\n",
      "4000/4000 [==============================] - 0s 55us/step - loss: 0.0873 - mae: 0.1841 - val_loss: 0.1336 - val_mae: 0.2281\n",
      "Epoch 57/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0872 - mae: 0.1839 - val_loss: 0.1336 - val_mae: 0.2267\n",
      "Epoch 58/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0871 - mae: 0.1835 - val_loss: 0.1342 - val_mae: 0.2287\n",
      "Epoch 59/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0871 - mae: 0.1837 - val_loss: 0.1331 - val_mae: 0.2279\n",
      "Epoch 60/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0873 - mae: 0.1839 - val_loss: 0.1340 - val_mae: 0.2296\n",
      "Epoch 61/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0869 - mae: 0.1832 - val_loss: 0.1341 - val_mae: 0.2290\n",
      "Epoch 62/200\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 0.0870 - mae: 0.1833 - val_loss: 0.1322 - val_mae: 0.2247\n",
      "Epoch 63/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0870 - mae: 0.1830 - val_loss: 0.1326 - val_mae: 0.2266\n",
      "Epoch 64/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0870 - mae: 0.1832 - val_loss: 0.1330 - val_mae: 0.2267\n",
      "Epoch 65/200\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.0869 - mae: 0.1828 - val_loss: 0.1332 - val_mae: 0.2266\n",
      "Epoch 66/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0869 - mae: 0.1828 - val_loss: 0.1347 - val_mae: 0.2288\n",
      "Epoch 67/200\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.0868 - mae: 0.1826 - val_loss: 0.1330 - val_mae: 0.2269\n",
      "Epoch 68/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0868 - mae: 0.1826 - val_loss: 0.1339 - val_mae: 0.2272\n",
      "Epoch 69/200\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 0.0868 - mae: 0.1826 - val_loss: 0.1350 - val_mae: 0.2321\n",
      "Epoch 70/200\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.0869 - mae: 0.1826 - val_loss: 0.1340 - val_mae: 0.2280\n",
      "Epoch 71/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0867 - mae: 0.1823 - val_loss: 0.1331 - val_mae: 0.2273\n",
      "Epoch 72/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0867 - mae: 0.1823 - val_loss: 0.1344 - val_mae: 0.2300\n",
      "Epoch 73/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0867 - mae: 0.1824 - val_loss: 0.1335 - val_mae: 0.2256\n",
      "Epoch 74/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0866 - mae: 0.1821 - val_loss: 0.1337 - val_mae: 0.2278\n",
      "Epoch 75/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0866 - mae: 0.1821 - val_loss: 0.1332 - val_mae: 0.2259\n",
      "Epoch 76/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0866 - mae: 0.1820 - val_loss: 0.1334 - val_mae: 0.2259\n",
      "Epoch 77/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0866 - mae: 0.1817 - val_loss: 0.1333 - val_mae: 0.2286\n",
      "Epoch 78/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0866 - mae: 0.1820 - val_loss: 0.1331 - val_mae: 0.2266\n",
      "Epoch 79/200\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.0865 - mae: 0.1816 - val_loss: 0.1346 - val_mae: 0.2312\n",
      "Epoch 80/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0866 - mae: 0.1818 - val_loss: 0.1330 - val_mae: 0.2279\n",
      "Epoch 81/200\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.0866 - mae: 0.1820 - val_loss: 0.1325 - val_mae: 0.2246\n",
      "Epoch 82/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0865 - mae: 0.1816 - val_loss: 0.1335 - val_mae: 0.2261\n",
      "Epoch 83/200\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.0865 - mae: 0.1817 - val_loss: 0.1327 - val_mae: 0.2259\n",
      "Epoch 84/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0864 - mae: 0.1813 - val_loss: 0.1330 - val_mae: 0.2253\n",
      "Epoch 85/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0863 - mae: 0.1811 - val_loss: 0.1334 - val_mae: 0.2266\n",
      "Epoch 86/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0863 - mae: 0.1809 - val_loss: 0.1336 - val_mae: 0.2278\n",
      "Epoch 87/200\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.0863 - mae: 0.1812 - val_loss: 0.1350 - val_mae: 0.2316\n",
      "Epoch 88/200\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.0863 - mae: 0.1812 - val_loss: 0.1342 - val_mae: 0.2277\n",
      "Epoch 89/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0865 - mae: 0.1815 - val_loss: 0.1349 - val_mae: 0.2299\n",
      "Epoch 90/200\n",
      "4000/4000 [==============================] - 0s 53us/step - loss: 0.0865 - mae: 0.1815 - val_loss: 0.1329 - val_mae: 0.2259\n",
      "Epoch 91/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0868 - mae: 0.1819 - val_loss: 0.1372 - val_mae: 0.2357\n",
      "Epoch 92/200\n",
      "4000/4000 [==============================] - 0s 52us/step - loss: 0.0864 - mae: 0.1812 - val_loss: 0.1359 - val_mae: 0.2309\n",
      "Epoch 93/200\n",
      "4000/4000 [==============================] - 0s 58us/step - loss: 0.0864 - mae: 0.1813 - val_loss: 0.1351 - val_mae: 0.2288\n",
      "Epoch 94/200\n",
      "4000/4000 [==============================] - 0s 52us/step - loss: 0.0863 - mae: 0.1809 - val_loss: 0.1352 - val_mae: 0.2279\n",
      "Epoch 95/200\n",
      "4000/4000 [==============================] - 0s 52us/step - loss: 0.0863 - mae: 0.1809 - val_loss: 0.1342 - val_mae: 0.2275\n",
      "Epoch 96/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0863 - mae: 0.1808 - val_loss: 0.1347 - val_mae: 0.2282\n",
      "Epoch 97/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0861 - mae: 0.1804 - val_loss: 0.1346 - val_mae: 0.2279\n",
      "Epoch 98/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0862 - mae: 0.1806 - val_loss: 0.1355 - val_mae: 0.2303\n",
      "Epoch 99/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0862 - mae: 0.1803 - val_loss: 0.1341 - val_mae: 0.2275\n",
      "Epoch 100/200\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.0861 - mae: 0.1803 - val_loss: 0.1332 - val_mae: 0.2271\n",
      "Epoch 101/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0861 - mae: 0.1802 - val_loss: 0.1322 - val_mae: 0.2244\n",
      "Epoch 102/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0861 - mae: 0.1805 - val_loss: 0.1326 - val_mae: 0.2265\n",
      "Epoch 103/200\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.0861 - mae: 0.1804 - val_loss: 0.1333 - val_mae: 0.2262\n",
      "Epoch 104/200\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 0.0862 - mae: 0.1806 - val_loss: 0.1330 - val_mae: 0.2239\n",
      "Epoch 105/200\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 0.0861 - mae: 0.1803 - val_loss: 0.1341 - val_mae: 0.2285\n",
      "Epoch 106/200\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 0.0861 - mae: 0.1803 - val_loss: 0.1330 - val_mae: 0.2250\n",
      "Epoch 107/200\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 0.0862 - mae: 0.1807 - val_loss: 0.1343 - val_mae: 0.2294\n",
      "Epoch 108/200\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 0.0863 - mae: 0.1809 - val_loss: 0.1346 - val_mae: 0.2294\n",
      "Epoch 109/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0862 - mae: 0.1807 - val_loss: 0.1342 - val_mae: 0.2290\n",
      "Epoch 110/200\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.0861 - mae: 0.1802 - val_loss: 0.1341 - val_mae: 0.2297\n",
      "Epoch 111/200\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.0860 - mae: 0.1800 - val_loss: 0.1337 - val_mae: 0.2280\n",
      "Epoch 112/200\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 0.0859 - mae: 0.1797 - val_loss: 0.1332 - val_mae: 0.2266\n",
      "Epoch 113/200\n",
      "4000/4000 [==============================] - 0s 66us/step - loss: 0.0859 - mae: 0.1798 - val_loss: 0.1341 - val_mae: 0.2280\n",
      "Epoch 114/200\n",
      "4000/4000 [==============================] - 0s 69us/step - loss: 0.0859 - mae: 0.1797 - val_loss: 0.1338 - val_mae: 0.2288\n",
      "Epoch 115/200\n",
      "4000/4000 [==============================] - 0s 61us/step - loss: 0.0859 - mae: 0.1795 - val_loss: 0.1333 - val_mae: 0.2271\n",
      "Epoch 116/200\n",
      "4000/4000 [==============================] - 0s 63us/step - loss: 0.0859 - mae: 0.1797 - val_loss: 0.1339 - val_mae: 0.2277\n",
      "Epoch 117/200\n",
      "4000/4000 [==============================] - 0s 57us/step - loss: 0.0859 - mae: 0.1798 - val_loss: 0.1333 - val_mae: 0.2277\n",
      "Epoch 118/200\n",
      "4000/4000 [==============================] - 0s 56us/step - loss: 0.0862 - mae: 0.1806 - val_loss: 0.1354 - val_mae: 0.2321\n",
      "Epoch 119/200\n",
      "4000/4000 [==============================] - 0s 53us/step - loss: 0.0861 - mae: 0.1803 - val_loss: 0.1341 - val_mae: 0.2289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0860 - mae: 0.1798 - val_loss: 0.1326 - val_mae: 0.2256\n",
      "Epoch 121/200\n",
      "4000/4000 [==============================] - 0s 55us/step - loss: 0.0859 - mae: 0.1798 - val_loss: 0.1334 - val_mae: 0.2274\n",
      "Epoch 122/200\n",
      "4000/4000 [==============================] - 0s 52us/step - loss: 0.0860 - mae: 0.1798 - val_loss: 0.1344 - val_mae: 0.2286\n",
      "Epoch 123/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0859 - mae: 0.1797 - val_loss: 0.1339 - val_mae: 0.2287\n",
      "Epoch 124/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0859 - mae: 0.1796 - val_loss: 0.1347 - val_mae: 0.2306\n",
      "Epoch 125/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0859 - mae: 0.1797 - val_loss: 0.1338 - val_mae: 0.2279\n",
      "Epoch 126/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0859 - mae: 0.1797 - val_loss: 0.1342 - val_mae: 0.2285\n",
      "Epoch 127/200\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.0859 - mae: 0.1796 - val_loss: 0.1335 - val_mae: 0.2272\n",
      "Epoch 128/200\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.0860 - mae: 0.1800 - val_loss: 0.1350 - val_mae: 0.2301\n",
      "Epoch 129/200\n",
      "4000/4000 [==============================] - 0s 39us/step - loss: 0.0860 - mae: 0.1799 - val_loss: 0.1349 - val_mae: 0.2298\n",
      "Epoch 130/200\n",
      "4000/4000 [==============================] - 0s 52us/step - loss: 0.0859 - mae: 0.1795 - val_loss: 0.1332 - val_mae: 0.2267\n",
      "Epoch 131/200\n",
      "4000/4000 [==============================] - 0s 62us/step - loss: 0.0859 - mae: 0.1797 - val_loss: 0.1333 - val_mae: 0.2264\n",
      "Epoch 132/200\n",
      "4000/4000 [==============================] - 0s 53us/step - loss: 0.0858 - mae: 0.1794 - val_loss: 0.1333 - val_mae: 0.2252\n",
      "Epoch 133/200\n",
      "4000/4000 [==============================] - 0s 54us/step - loss: 0.0858 - mae: 0.1795 - val_loss: 0.1328 - val_mae: 0.2257\n",
      "Epoch 134/200\n",
      "4000/4000 [==============================] - 0s 54us/step - loss: 0.0858 - mae: 0.1793 - val_loss: 0.1349 - val_mae: 0.2303\n",
      "Epoch 135/200\n",
      "4000/4000 [==============================] - 0s 56us/step - loss: 0.0858 - mae: 0.1794 - val_loss: 0.1346 - val_mae: 0.2297\n",
      "Epoch 136/200\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 0.0858 - mae: 0.1793 - val_loss: 0.1326 - val_mae: 0.2248\n",
      "Epoch 137/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0858 - mae: 0.1793 - val_loss: 0.1339 - val_mae: 0.2281\n",
      "Epoch 138/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0858 - mae: 0.1795 - val_loss: 0.1351 - val_mae: 0.2287\n",
      "Epoch 139/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0857 - mae: 0.1792 - val_loss: 0.1333 - val_mae: 0.2255\n",
      "Epoch 140/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0858 - mae: 0.1795 - val_loss: 0.1357 - val_mae: 0.2316\n",
      "Epoch 141/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0858 - mae: 0.1793 - val_loss: 0.1344 - val_mae: 0.2306\n",
      "Epoch 142/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0859 - mae: 0.1795 - val_loss: 0.1338 - val_mae: 0.2284\n",
      "Epoch 143/200\n",
      "4000/4000 [==============================] - 0s 50us/step - loss: 0.0859 - mae: 0.1797 - val_loss: 0.1327 - val_mae: 0.2234\n",
      "Epoch 144/200\n",
      "4000/4000 [==============================] - 0s 57us/step - loss: 0.0859 - mae: 0.1794 - val_loss: 0.1330 - val_mae: 0.2277\n",
      "Epoch 145/200\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 0.0857 - mae: 0.1790 - val_loss: 0.1337 - val_mae: 0.2306\n",
      "Epoch 146/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0857 - mae: 0.1788 - val_loss: 0.1336 - val_mae: 0.2277\n",
      "Epoch 147/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0857 - mae: 0.1790 - val_loss: 0.1331 - val_mae: 0.2277\n",
      "Epoch 148/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0856 - mae: 0.1787 - val_loss: 0.1340 - val_mae: 0.2265\n",
      "Epoch 149/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0858 - mae: 0.1794 - val_loss: 0.1327 - val_mae: 0.2257\n",
      "Epoch 150/200\n",
      "4000/4000 [==============================] - 0s 52us/step - loss: 0.0857 - mae: 0.1789 - val_loss: 0.1336 - val_mae: 0.2281\n",
      "Epoch 151/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0856 - mae: 0.1787 - val_loss: 0.1328 - val_mae: 0.2265\n",
      "Epoch 152/200\n",
      "4000/4000 [==============================] - 0s 44us/step - loss: 0.0856 - mae: 0.1788 - val_loss: 0.1335 - val_mae: 0.2283\n",
      "Epoch 153/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0858 - mae: 0.1793 - val_loss: 0.1361 - val_mae: 0.2326\n",
      "Epoch 154/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0856 - mae: 0.1786 - val_loss: 0.1333 - val_mae: 0.2272\n",
      "Epoch 155/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0857 - mae: 0.1791 - val_loss: 0.1338 - val_mae: 0.2295\n",
      "Epoch 156/200\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 0.0856 - mae: 0.1787 - val_loss: 0.1338 - val_mae: 0.2293\n",
      "Epoch 157/200\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 0.0856 - mae: 0.1787 - val_loss: 0.1332 - val_mae: 0.2273\n",
      "Epoch 158/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0857 - mae: 0.1789 - val_loss: 0.1335 - val_mae: 0.2275\n",
      "Epoch 159/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0857 - mae: 0.1789 - val_loss: 0.1389 - val_mae: 0.2337\n",
      "Epoch 160/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0856 - mae: 0.1787 - val_loss: 0.1360 - val_mae: 0.2302\n",
      "Epoch 161/200\n",
      "4000/4000 [==============================] - 0s 41us/step - loss: 0.0856 - mae: 0.1786 - val_loss: 0.1344 - val_mae: 0.2275\n",
      "Epoch 162/200\n",
      "4000/4000 [==============================] - 0s 40us/step - loss: 0.0856 - mae: 0.1786 - val_loss: 0.1356 - val_mae: 0.2317\n",
      "Epoch 163/200\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.0855 - mae: 0.1784 - val_loss: 0.1357 - val_mae: 0.2316\n",
      "Epoch 164/200\n",
      "4000/4000 [==============================] - 0s 53us/step - loss: 0.0855 - mae: 0.1782 - val_loss: 0.1345 - val_mae: 0.2319\n",
      "Epoch 165/200\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.0855 - mae: 0.1784 - val_loss: 0.1338 - val_mae: 0.2297\n",
      "Epoch 166/200\n",
      "4000/4000 [==============================] - 0s 53us/step - loss: 0.0859 - mae: 0.1794 - val_loss: 0.1360 - val_mae: 0.2338\n",
      "Epoch 167/200\n",
      "4000/4000 [==============================] - 0s 61us/step - loss: 0.0858 - mae: 0.1790 - val_loss: 0.1350 - val_mae: 0.2304\n",
      "Epoch 168/200\n",
      "4000/4000 [==============================] - 0s 66us/step - loss: 0.0857 - mae: 0.1788 - val_loss: 0.1346 - val_mae: 0.2286\n",
      "Epoch 169/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0855 - mae: 0.1784 - val_loss: 0.1340 - val_mae: 0.2282\n",
      "Epoch 170/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0856 - mae: 0.1785 - val_loss: 0.1344 - val_mae: 0.2287\n",
      "Epoch 171/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0856 - mae: 0.1786 - val_loss: 0.1354 - val_mae: 0.2306\n",
      "Epoch 172/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0856 - mae: 0.1785 - val_loss: 0.1364 - val_mae: 0.2315\n",
      "Epoch 173/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0855 - mae: 0.1782 - val_loss: 0.1346 - val_mae: 0.2313\n",
      "Epoch 174/200\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 0.0855 - mae: 0.1781 - val_loss: 0.1344 - val_mae: 0.2294\n",
      "Epoch 175/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0855 - mae: 0.1782 - val_loss: 0.1329 - val_mae: 0.2257\n",
      "Epoch 176/200\n",
      "4000/4000 [==============================] - 0s 48us/step - loss: 0.0855 - mae: 0.1782 - val_loss: 0.1359 - val_mae: 0.2331\n",
      "Epoch 177/200\n",
      "4000/4000 [==============================] - 0s 52us/step - loss: 0.0854 - mae: 0.1781 - val_loss: 0.1357 - val_mae: 0.2333\n",
      "Epoch 178/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0855 - mae: 0.1781 - val_loss: 0.1351 - val_mae: 0.2284\n",
      "Epoch 179/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0856 - mae: 0.1784 - val_loss: 0.1349 - val_mae: 0.2303\n",
      "Epoch 180/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0854 - mae: 0.1780 - val_loss: 0.1342 - val_mae: 0.2263\n",
      "Epoch 181/200\n",
      "4000/4000 [==============================] - 0s 49us/step - loss: 0.0854 - mae: 0.1778 - val_loss: 0.1346 - val_mae: 0.2311\n",
      "Epoch 182/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0853 - mae: 0.1775 - val_loss: 0.1336 - val_mae: 0.2276\n",
      "Epoch 183/200\n",
      "4000/4000 [==============================] - 0s 54us/step - loss: 0.0854 - mae: 0.1778 - val_loss: 0.1343 - val_mae: 0.2302\n",
      "Epoch 184/200\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.0854 - mae: 0.1779 - val_loss: 0.1351 - val_mae: 0.2311\n",
      "Epoch 185/200\n",
      "4000/4000 [==============================] - 0s 53us/step - loss: 0.0853 - mae: 0.1775 - val_loss: 0.1393 - val_mae: 0.2386\n",
      "Epoch 186/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0854 - mae: 0.1778 - val_loss: 0.1358 - val_mae: 0.2327\n",
      "Epoch 187/200\n",
      "4000/4000 [==============================] - 0s 59us/step - loss: 0.0855 - mae: 0.1783 - val_loss: 0.1377 - val_mae: 0.2359\n",
      "Epoch 188/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0857 - mae: 0.1791 - val_loss: 0.1388 - val_mae: 0.2337\n",
      "Epoch 189/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0854 - mae: 0.1778 - val_loss: 0.1372 - val_mae: 0.2318\n",
      "Epoch 190/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0855 - mae: 0.1781 - val_loss: 0.1363 - val_mae: 0.2309\n",
      "Epoch 191/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0853 - mae: 0.1775 - val_loss: 0.1360 - val_mae: 0.2326\n",
      "Epoch 192/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0853 - mae: 0.1776 - val_loss: 0.1348 - val_mae: 0.2298\n",
      "Epoch 193/200\n",
      "4000/4000 [==============================] - 0s 51us/step - loss: 0.0855 - mae: 0.1783 - val_loss: 0.1365 - val_mae: 0.2334\n",
      "Epoch 194/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0854 - mae: 0.1780 - val_loss: 0.1361 - val_mae: 0.2311\n",
      "Epoch 195/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0853 - mae: 0.1775 - val_loss: 0.1352 - val_mae: 0.2308\n",
      "Epoch 196/200\n",
      "4000/4000 [==============================] - 0s 46us/step - loss: 0.0854 - mae: 0.1779 - val_loss: 0.1352 - val_mae: 0.2316\n",
      "Epoch 197/200\n",
      "4000/4000 [==============================] - 0s 43us/step - loss: 0.0857 - mae: 0.1787 - val_loss: 0.1344 - val_mae: 0.2309\n",
      "Epoch 198/200\n",
      "4000/4000 [==============================] - 0s 45us/step - loss: 0.0853 - mae: 0.1778 - val_loss: 0.1344 - val_mae: 0.2295\n",
      "Epoch 199/200\n",
      "4000/4000 [==============================] - 0s 47us/step - loss: 0.0853 - mae: 0.1776 - val_loss: 0.1339 - val_mae: 0.2295\n",
      "Epoch 200/200\n",
      "4000/4000 [==============================] - 0s 42us/step - loss: 0.0852 - mae: 0.1772 - val_loss: 0.1358 - val_mae: 0.2332\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_input = train.drop('outlier', axis=1)\n",
    "test_input = test.drop('outlier', axis=1)\n",
    "autoencoder_model = fit_autoencoder_2(train_input, test_input, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_model = load_model('model_autoencoder.h5')\n",
    "reconstructed_data = autoencoder_model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>9.999238e-01</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.006152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009311</td>\n",
       "      <td>0.438299</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.980232e-08</td>\n",
       "      <td>0.158436</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.196576</td>\n",
       "      <td>0.519492</td>\n",
       "      <td>4.985393e-01</td>\n",
       "      <td>0.260360</td>\n",
       "      <td>0.264625</td>\n",
       "      <td>0.068176</td>\n",
       "      <td>0.093200</td>\n",
       "      <td>0.464659</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.089028e-01</td>\n",
       "      <td>0.999701</td>\n",
       "      <td>0.741399</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996700</td>\n",
       "      <td>8.940697e-08</td>\n",
       "      <td>0.670698</td>\n",
       "      <td>0.998844</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.373530</td>\n",
       "      <td>0.346195</td>\n",
       "      <td>2.980232e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.542536e-01</td>\n",
       "      <td>0.999960</td>\n",
       "      <td>0.805894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998966</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.709855</td>\n",
       "      <td>0.999159</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.289760</td>\n",
       "      <td>0.357156</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.437680e-01</td>\n",
       "      <td>0.979407</td>\n",
       "      <td>0.568554</td>\n",
       "      <td>0.999991</td>\n",
       "      <td>0.997990</td>\n",
       "      <td>8.940697e-08</td>\n",
       "      <td>0.766612</td>\n",
       "      <td>0.897230</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.112497</td>\n",
       "      <td>0.367678</td>\n",
       "      <td>1.192093e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4             5   \\\n",
       "0  0.000000e+00  0.000008  0.000000  0.000000  0.000895  9.999238e-01   \n",
       "1  2.980232e-08  0.158436  0.000014  0.196576  0.519492  4.985393e-01   \n",
       "2  3.089028e-01  0.999701  0.741399  1.000000  0.996700  8.940697e-08   \n",
       "3  3.542536e-01  0.999960  0.805894  1.000000  0.998966  0.000000e+00   \n",
       "4  2.437680e-01  0.979407  0.568554  0.999991  0.997990  8.940697e-08   \n",
       "\n",
       "         6         7         8         9         10            11  \n",
       "0  0.000147  0.006152  0.000000  0.009311  0.438299  0.000000e+00  \n",
       "1  0.260360  0.264625  0.068176  0.093200  0.464659  0.000000e+00  \n",
       "2  0.670698  0.998844  1.000000  0.373530  0.346195  2.980232e-08  \n",
       "3  0.709855  0.999159  1.000000  0.289760  0.357156  0.000000e+00  \n",
       "4  0.766612  0.897230  1.000000  0.112497  0.367678  1.192093e-07  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(reconstructed_data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>-0.634121</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>-1.152175</td>\n",
       "      <td>-0.443850</td>\n",
       "      <td>-0.104699</td>\n",
       "      <td>1.197655</td>\n",
       "      <td>-0.126572</td>\n",
       "      <td>-0.052289</td>\n",
       "      <td>-0.557117</td>\n",
       "      <td>0.082802</td>\n",
       "      <td>0.408213</td>\n",
       "      <td>-0.335326</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>-0.345560</td>\n",
       "      <td>0.353255</td>\n",
       "      <td>-0.332076</td>\n",
       "      <td>0.055558</td>\n",
       "      <td>0.444000</td>\n",
       "      <td>0.383816</td>\n",
       "      <td>0.334197</td>\n",
       "      <td>0.202975</td>\n",
       "      <td>0.004369</td>\n",
       "      <td>0.039851</td>\n",
       "      <td>0.488745</td>\n",
       "      <td>-0.300753</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>0.328938</td>\n",
       "      <td>1.235516</td>\n",
       "      <td>0.594167</td>\n",
       "      <td>1.235576</td>\n",
       "      <td>1.040672</td>\n",
       "      <td>-0.385087</td>\n",
       "      <td>0.738479</td>\n",
       "      <td>0.994296</td>\n",
       "      <td>1.297051</td>\n",
       "      <td>0.049622</td>\n",
       "      <td>0.489813</td>\n",
       "      <td>-0.289947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>0.455620</td>\n",
       "      <td>1.150332</td>\n",
       "      <td>0.663968</td>\n",
       "      <td>1.370372</td>\n",
       "      <td>0.966643</td>\n",
       "      <td>-0.469966</td>\n",
       "      <td>0.729252</td>\n",
       "      <td>1.043847</td>\n",
       "      <td>1.448706</td>\n",
       "      <td>0.073479</td>\n",
       "      <td>0.480137</td>\n",
       "      <td>-0.282926</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>0.423580</td>\n",
       "      <td>0.984021</td>\n",
       "      <td>0.453787</td>\n",
       "      <td>0.994711</td>\n",
       "      <td>1.002812</td>\n",
       "      <td>-0.292719</td>\n",
       "      <td>0.735419</td>\n",
       "      <td>0.721012</td>\n",
       "      <td>1.289829</td>\n",
       "      <td>0.114508</td>\n",
       "      <td>0.446843</td>\n",
       "      <td>-0.277770</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6  \\\n",
       "4193 -0.634121  0.002379 -1.152175 -0.443850 -0.104699  1.197655 -0.126572   \n",
       "4194 -0.345560  0.353255 -0.332076  0.055558  0.444000  0.383816  0.334197   \n",
       "4195  0.328938  1.235516  0.594167  1.235576  1.040672 -0.385087  0.738479   \n",
       "4196  0.455620  1.150332  0.663968  1.370372  0.966643 -0.469966  0.729252   \n",
       "4197  0.423580  0.984021  0.453787  0.994711  1.002812 -0.292719  0.735419   \n",
       "\n",
       "             7         8         9        10        11  outlier  \n",
       "4193 -0.052289 -0.557117  0.082802  0.408213 -0.335326      0.0  \n",
       "4194  0.202975  0.004369  0.039851  0.488745 -0.300753      0.0  \n",
       "4195  0.994296  1.297051  0.049622  0.489813 -0.289947      0.0  \n",
       "4196  1.043847  1.448706  0.073479  0.480137 -0.282926      0.0  \n",
       "4197  0.721012  1.289829  0.114508  0.446843 -0.277770      0.0  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse        364\n",
       "outlier    364\n",
       "dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = np.mean(np.power(test_input - reconstructed_data, 2), axis=1)\n",
    "results = pd.DataFrame()\n",
    "results['mse'] = mse\n",
    "results['outlier'] = test.outlier\n",
    "results[results['mse'] > 0.1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reconstruction_error</th>\n",
       "      <th>true_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>923.000000</td>\n",
       "      <td>923.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.135777</td>\n",
       "      <td>0.258938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.192974</td>\n",
       "      <td>0.438289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.034843</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.080342</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.159185</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.559870</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reconstruction_error  true_class\n",
       "count            923.000000  923.000000\n",
       "mean               0.135777    0.258938\n",
       "std                0.192974    0.438289\n",
       "min                0.002675    0.000000\n",
       "25%                0.034843    0.000000\n",
       "50%                0.080342    0.000000\n",
       "75%                0.159185    1.000000\n",
       "max                2.559870    1.000000"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
    "                        'true_class': test.outlier})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reconstruction error without fraud\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "normal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 2)]\n",
    "_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Reconstruction error with fraud\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fraud_error_df = error_df[error_df['true_class'] == 1]\n",
    "_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4193</th>\n",
       "      <td>0.202123</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4194</th>\n",
       "      <td>0.034467</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4195</th>\n",
       "      <td>0.048823</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4196</th>\n",
       "      <td>0.063148</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4197</th>\n",
       "      <td>0.027560</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4198</th>\n",
       "      <td>0.016390</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4199</th>\n",
       "      <td>0.018345</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4200</th>\n",
       "      <td>0.014709</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4201</th>\n",
       "      <td>0.019864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4202</th>\n",
       "      <td>0.026331</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4203</th>\n",
       "      <td>0.018575</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4204</th>\n",
       "      <td>0.088484</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>0.106391</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>0.197526</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>0.144710</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>0.086978</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4209</th>\n",
       "      <td>0.286511</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4210</th>\n",
       "      <td>0.258795</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4211</th>\n",
       "      <td>0.304575</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4212</th>\n",
       "      <td>0.255654</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4213</th>\n",
       "      <td>0.258604</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4214</th>\n",
       "      <td>0.441041</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4216</th>\n",
       "      <td>0.128825</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4217</th>\n",
       "      <td>0.096658</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4218</th>\n",
       "      <td>0.080343</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4219</th>\n",
       "      <td>0.096789</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4220</th>\n",
       "      <td>0.214215</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4221</th>\n",
       "      <td>0.355922</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4222</th>\n",
       "      <td>0.140452</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4223</th>\n",
       "      <td>0.289094</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>0.121319</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>0.159514</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>0.166757</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4267</th>\n",
       "      <td>0.139677</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4268</th>\n",
       "      <td>0.162577</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4269</th>\n",
       "      <td>0.237810</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>0.084979</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>0.173038</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4350</th>\n",
       "      <td>0.094891</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4351</th>\n",
       "      <td>0.133453</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4371</th>\n",
       "      <td>0.052997</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4372</th>\n",
       "      <td>0.046145</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4373</th>\n",
       "      <td>0.033084</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4391</th>\n",
       "      <td>0.061605</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4392</th>\n",
       "      <td>0.100639</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4393</th>\n",
       "      <td>0.206531</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4394</th>\n",
       "      <td>0.228819</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4395</th>\n",
       "      <td>0.227734</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4396</th>\n",
       "      <td>0.126871</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4397</th>\n",
       "      <td>0.110246</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4416</th>\n",
       "      <td>0.068973</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4417</th>\n",
       "      <td>0.058759</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4418</th>\n",
       "      <td>0.049368</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4419</th>\n",
       "      <td>0.078359</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4551</th>\n",
       "      <td>0.102007</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>0.086150</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4730</th>\n",
       "      <td>0.069554</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4731</th>\n",
       "      <td>0.081047</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>0.052610</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>0.058341</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>923 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           mse  outlier\n",
       "4193  0.202123      0.0\n",
       "4194  0.034467      0.0\n",
       "4195  0.048823      0.0\n",
       "4196  0.063148      0.0\n",
       "4197  0.027560      0.0\n",
       "4198  0.016390      0.0\n",
       "4199  0.018345      0.0\n",
       "4200  0.014709      0.0\n",
       "4201  0.019864      0.0\n",
       "4202  0.026331      0.0\n",
       "4203  0.018575      0.0\n",
       "4204  0.088484      0.0\n",
       "4205  0.106391      0.0\n",
       "4206  0.197526      0.0\n",
       "4207  0.144710      0.0\n",
       "4208  0.086978      0.0\n",
       "4209  0.286511      0.0\n",
       "4210  0.258795      0.0\n",
       "4211  0.304575      0.0\n",
       "4212  0.255654      0.0\n",
       "4213  0.258604      0.0\n",
       "4214  0.441041      0.0\n",
       "4216  0.128825      0.0\n",
       "4217  0.096658      0.0\n",
       "4218  0.080343      0.0\n",
       "4219  0.096789      0.0\n",
       "4220  0.214215      0.0\n",
       "4221  0.355922      0.0\n",
       "4222  0.140452      0.0\n",
       "4223  0.289094      0.0\n",
       "...        ...      ...\n",
       "4264  0.121319      1.0\n",
       "4265  0.159514      1.0\n",
       "4266  0.166757      1.0\n",
       "4267  0.139677      1.0\n",
       "4268  0.162577      1.0\n",
       "4269  0.237810      1.0\n",
       "4279  0.084979      1.0\n",
       "4289  0.173038      1.0\n",
       "4350  0.094891      1.0\n",
       "4351  0.133453      1.0\n",
       "4371  0.052997      1.0\n",
       "4372  0.046145      1.0\n",
       "4373  0.033084      1.0\n",
       "4391  0.061605      1.0\n",
       "4392  0.100639      1.0\n",
       "4393  0.206531      1.0\n",
       "4394  0.228819      1.0\n",
       "4395  0.227734      1.0\n",
       "4396  0.126871      1.0\n",
       "4397  0.110246      1.0\n",
       "4416  0.068973      1.0\n",
       "4417  0.058759      1.0\n",
       "4418  0.049368      1.0\n",
       "4419  0.078359      1.0\n",
       "4551  0.102007      1.0\n",
       "4675  0.086150      1.0\n",
       "4730  0.069554      1.0\n",
       "4731  0.081047      1.0\n",
       "4752  0.052610      1.0\n",
       "4753  0.058341      1.0\n",
       "\n",
       "[923 rows x 2 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
